##
# Copyright(c) 2010-2015 Intel Corporation.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#
#   * Redistributions of source code must retain the above copyright
#     notice, this list of conditions and the following disclaimer.
#   * Redistributions in binary form must reproduce the above copyright
#     notice, this list of conditions and the following disclaimer in
#     the documentation and/or other materials provided with the
#     distribution.
#   * Neither the name of Intel Corporation nor the names of its
#     contributors may be used to endorse or promote products derived
#     from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
##

Description
-----------

Intel(r) DPDK based Prototype Applications implementing Telco
use-cases like a simplified BRAS/BNG, light-weight AFTR, but also
allows to configure other finer grained network functions like QoS,
Routing, load-balancing, ...

Compiling and running this application
--------------------------------------

This application has mainly been tested with DPDK version 1.7.1 and
DPDK 1.8.0, but DPDK version 2.0.0 is also supported. Depending on the
DPDK version, the installation procedure is slightly different. The
following commands assume that the following variables have been set:

export DPPD_DIR=/path/to/this/application
export RTE_SDK=/path/to/dpdk
export RTE_TARGET=x86_64-native-linuxapp-gcc

DPDK 1.7.1 installation
-----------------------

git clone http://dpdk.org/git/dpdk
cd dpdk
git checkout v1.7.1
patch -p1 < $DPPD_DIR/dpdk-patches/*-DPDK-1.7.1-*.patch
git cherry-pick 98f255ed0a4a73bf785e884dc2069405de840546
git cherry-pick 0d6e2d783d164503e75e57b7358dc3c5c09e0bf1
git cherry-pick 442f3bed6a0964e938c809e8811fa0636ef13f49
git cherry-pick ee19d51ae56dcb685d9a04d9e859873d35565232
git cherry-pick 6573369dd735c43930bfddec34124da05b1584df
git cherry-pick 81ab433fffa4885a76023abd3c8a7e47aee014e3
git cherry-pick 2f95a470b80df115a0a0c9fba4bfbae73f4d66ae
make install T=$RTE_TARGET

DPDK 1.8.0 installation
-----------------------
git clone http://dpdk.org/git/dpdk
cd dpdk
git checkout v1.8.0
git cherry-pick 2f95a470b80df115a0a0c9fba4bfbae73f4d66ae
make install T=$RTE_TARGET

DPDK 2.0.0 installation
-----------------------
git clone http://dpdk.org/git/dpdk
cd dpdk
git checkout v2.0.0
make install T=$RTE_TARGET

DPPD compilation
----------------

The Makefile with this application expects RTE_SDK to point to the
root directory of DPDK (i.e. export RTE_SDK=/root/dpdk). If RTE_TARGET
has not been set, x86_64-native-linuxapp-gcc will be assumed.

Running DPPD
------------
After DPDK has been set up, run make from the directory where you have
extracted this application. A build directory will be created
containing the dppd executable. The usage of the application is shown
below. Note that this application assumes that all required ports have
been bound to the DPDK provided igb_uio driver. Refer to the "Getting
Started Guide - DPDK" document for more details.

Usage: ./build/dppd [-f CONFIG_FILE] [-a|-e] [-s|-i] [-w DEF] [-u] [-t]
	-f CONFIG_FILE : configuration file to load, ./dppd.cfg by default
	-l LOG_FILE : log file name, ./dppd.log by default
	-p : include PID in log file name if default log file is used
	-a : autostart all cores (by default)
	-e : don't autostart
	-s : check configuration file syntax and exit
	-i : check initialization sequence and exit
	-u : Listen on UDS /tmp/dppd.sock
	-t : Listen on TCP port 8474
	-w : define variable using syntax varname=value
	     takes precedence over variables defined in CONFIG_FILE

While applications using DPDK typically rely on the core mask and the
number of channels to be specified on the command line, this
application is configured using a .cfg file. The core mask and number
of channels is derived from this config. For example, to run the
application from the source directory execute:

  user@target:~$ ./build/dppd -f ./config/handle_none.cfg

Provided example configurations
-------------------------------

A few example configurations are provided with the source in the
config directory. Pktgen scripts are provided to generate traffic for
most of these configurations. The list of example applications is
shown below:

- handle_none.cfg

	This is one of the most basic configurations. Note that this
	configuration does not perform any real work as opposed to
	configurations like bng.cfg and lwaftr.cfg. This configuration
	sets up four interfaces and five cores (one master core and
	four worker cores). Packets are passed (i.e. without being
	touched) as follows:

		- interface 0 to interface 1 (handled by core 1)
		- interface 1 to interface 0 (handled by core 2)
		- interface 2 to interface 3 (handled by core 3)
		- interface 3 to interface 2 (handled by core 4)

- bng-4ports.cfg

	This configuration sets up a Border Network Gateway (BNG) on
	the first socket (socket 0). Four load balancers (two physical
	cores, four logical cores) and eight workers (four physical
	cores, eight logical cores) are set up. The number of workers
	can be changed by uncommenting one of the lines in the
	[variables] section. If this configuration is to be used on a
	system with few cores, the number of workers need to be
	reduced.

- bng-8ports.cfg

	This configuration sets up a system that handles the same
	workload as bng-4ports.cfg, but on 8 ports instead of 4 and
	on CPU socket 1 instead of socket 0.

- bng-qos-4ports.cfg

	Compared to bng-4ports.cfg, this configuration sets up a BNG with QoS
	functionality. In total, an extra eight cores (four physical
	cores) are needed to run this configuration. Four cores are
	used for QoS, two cores are assigned with the task of
	classifying upstream packets and two cores are assigned with
	transmitting downstream packets.

- bng-qos-8ports.cfg

	This configuration sets up a system that handles the same
	workload as bng-qos-4ports.cfg, but on 8 ports instead of 4 and
	on CPU socket 1 instead of socket 0.

- bng-1q-4ports.cfg

	This configuration sets up a system that handles the same
	workload as bng-4ports.cfg. The difference is that on each of the
	interfaces, only one queue is used. Use-cases for this
	configuration include running in a virtualized environment
	using SRIOV.

- bng-ovs-usv-4ports.cfg

	This configuration is provided for virtualized environments
	running on a top of a soft-switch. More specifically, the
	ingredients are Open vSwitch (openvswitch.org) and a Qemu
	version 1.6.2. Note that since the current supported version
	Open vSwitch does not handle all the protocols that are used
	in the full BNG (bng.cfg), dppd has to be recompiled to use
	different packet processing paths as a workaround. DPDK
	version 1.8.0 should be used with this configuration and it
	has to be compiled with COMBINE_LIBS=y:

	make install T=$RTE_TARGET CONFIG_RTE_BUILD_COMBINE_LIBS=y

	The following commands demonstrate how the set up Open
	vSwitch.

	git clone https://github.com/openvswitch/ovs.git
	cd ovs
	git checkout 5c62a855c7bb24424cbe7ec48ecf2f128db8b102
	./boot.sh && ./configure --with-dpdk=RTE_SDK/$RTE_TARGET --disable-ssl && make

	The bng-ovs-usv-4ports.cfg is intended to be used in a VM with
	4 virtual ports. This means that 4 virtual ports (with type
	dpdkvhost) and 4 physical ports (with type dpdk) will need to
	be add and connected through open-flow commands in Open
	vSwitch. After Open vSwitch has been set up on the host, the
	program needs to be recompiled in the VM as follows before
	running it with the bng-ovs-usv-4ports.cfg configuration:

		make BNG_QINQ=n MPLS_ROUTING=n

- handle_none-rings.cfg


	This configuration is similar to handle_none.cfg with the
	difference being the type of interfaces. The physical ports
	are replaced by DPDK rings. To use this functionality,
	RTE_TARGET needs to be set to x86_64-ivshmem-linuxapp-gcc
	before compiling DPDK (i.e. export
	RTE_TARGET=x86_64-ivshmem-linuxapp-gcc). Also, DPDK needs to
	be compiled with CONFIG_RTE_BUILD_COMBINE_LIBS=y and
	CONFIG_RTE_LIBRTE_VHOST=y The configuration can then be used
	inside a VM running on top of Open vSwitch. The SHA-1 of the
	version of Open vSwitch that has been tested is
	c78a00b112c9. To run the VM, Qemu needs to be patched to
	support ivshmem with multiple regions and the right command
	line arguments to be used to share memory. Download and patch
	Qemu 1.6.2 using the following commands:

	git clone git://git.qemu-project.org/qemu.git
	cd qemu
	git checkout git checkout v1.6.2
	wget https://01.org/sites/default/files/page/qemu-v1.6.2-ivshmem-dpdk.patch
	patch -p1 < qemu-v1.6.2-ivshmem-dpdk.patch
	./configure
	make

	After Open vSwitch has been configured with DPDK rings as
	ports (i.e. ports with type dpdkr), Qemu needs to be started
	with the correct command line arguments. Refer to Section 11.1
	from the DPDK programmer's Guide on how to build the Qemu
	command line arguments. The handle_none-rings.cfg
	configuration uses 4 ports. This means that 8 rings (4 for TX
	and 4 for RX) will need to be shared with the VM through
	ivshmem.

- lw_aftr.cfg

	This configuration creates the functionality of a lwAFTR component
	of the lw4over6 architecture as described in IETF draft
	http://tools.ietf.org/id/draft-ietf-softwire-lw4over6-13.txt.
	The lwAFTR simply terminates IPv6 tunnels that carry IPv4 traffic
	for many customers (one tunnel per customer).
	It consists of two tasks:
	1) ipv6_encap that encapsulates IPv4 packets into IPv6 and sends
	those tunnel packets towards the customer tunnel endpoint. For this,
	it must use a binding table that associates with each tunnel,
	a public IPv4 address and a set of ports.
	2) ipv6_decap which handles packet arriving from the tunnel, checks
	they use a source IPv4 address and port combination that matches
	their originating tunnel (based on the same binding table as used
	by ipv6_encap), removes the IPv6 encapsulation and sends them out
	its "internet" interface.

	By default, the binding table is in config/ipv6_tun_bind.csv.
	Binding tables of different sizes and different ranges of addresses
	and ports can be generated by a provided helper script:

		helper-scripts/ipv6_tun/ipv6_tun_bindings.pl -n <num_entries>

	Most other parameters of the generated binding table can be tweaked
	through script command line switches. For more details, refer to the
	documentation of the script obtained by running it with -help as
	argument.
	The same script can also generate tables for testing tools to generate
	packets with addresses and ports that match entries from the binding
	table (randomly selecting entries from the binding table).
